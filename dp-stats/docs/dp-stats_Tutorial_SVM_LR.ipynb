{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentially Private SVM Algorithm Tutorial\n",
    "\n",
    "\n",
    "The following tutorial gives one example of how a differentially private pipeline of PCA and SVM functions are called. The data set used is generated from randomly drawn samples from a mulitivariate Gaussian distribution. A non-differentially private pipeline of PCA and SVM is also utilized. This is used to compare the results (classification accuracy) of the differentially private pipeline.\n",
    "\n",
    "###### A sample of the pipeline is shown below:\n",
    "\n",
    "Generate samples --> Perform PCA for dimension reduction --> Perform SVM for classifier training --> Test classifier\n",
    "\n",
    "The parameters that can be adjusted are:\n",
    "\n",
    "- Epsilon_pca\n",
    "- Epsilon_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Non-private Quality: 0.9'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Differentially Private Quality: 0.895'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.svm_pipeline>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "from IPython.display import display\n",
    "\n",
    "# This tutorial shows one common pipeline for learning a binary classifier\n",
    "\n",
    "\n",
    "# This function is used to randomly generate data samples from a multivariate Gaussian distribution.\n",
    "def gen_data(num_tst_samp, k):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            num_tst_samp: total number of test samples to return in data matrix\n",
    "            n: number of samples to generate for each class\n",
    "    Outputs:\n",
    "            data: data matrix with samples in rows, [nxd]\n",
    "            labels: n dimensional vector\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import dp_stats as dps\n",
    "\n",
    "    d = 10    # features\n",
    "    n = num_tst_samp\n",
    "\n",
    "    # create covariance matrix\n",
    "    A = np.zeros((d,d))\n",
    "    for i in range(d):\n",
    "        if i < k:\n",
    "            A[i,i] = d - i\n",
    "        else:\n",
    "            A[i, i] = 1\n",
    "\n",
    "    # create mean for class 1\n",
    "    mean1 = -1 * np.ones(d)\n",
    "\n",
    "    # create mean for class 2\n",
    "    mean2 = np.ones(d)\n",
    "\n",
    "    # generate n samples for class 1\n",
    "    cls1_samps = np.random.multivariate_normal(mean1, A, n)    # [nxd]\n",
    "    # generate n samples for class 2\n",
    "    cls2_samps = np.random.multivariate_normal(mean2, A, n)    # [nxd]\n",
    "    return cls1_samps, cls2_samps, A\n",
    "\n",
    "# This function is used to randomly mix the two class samples and return training and testing data\n",
    "def sample_selection(data_cls1, data_cls2, trn_size, N):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            data_cls1: class 1 data, samples are in rows [nxd]\n",
    "            data_cls2: class 2 data, samples are in rows [nxd]\n",
    "            trn_size: number of samples to use for training data, integer\n",
    "            tst_size: number of samples to use for testing data, integer\n",
    "    Outputs:\n",
    "            trn_data: [trn_size x d]\n",
    "            trn_labels:[trn_size]\n",
    "            tst_data: [tst_size x d]\n",
    "            tst_labels: [tst_size]\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    ind = np.random.permutation(N)\n",
    "    trn_data1 = data_cls1[ind[:trn_size],:]\n",
    "    trn_data2 = data_cls2[ind[:trn_size],:]\n",
    "    tst_data1 = data_cls1[ind[trn_size:N],:]\n",
    "    tst_data2 = data_cls2[ind[trn_size:N],:]\n",
    "\n",
    "    trn_data = np.concatenate((trn_data1, trn_data2),axis=0)\n",
    "    trn_labels = np.concatenate( (np.ones(trn_size), -1*np.ones(trn_size)),axis=0 )\n",
    "\n",
    "    tst_data = np.concatenate((tst_data1, tst_data2),axis=0)\n",
    "    tst_labels = np.concatenate((np.ones(N-trn_size),-1*np.ones(N-trn_size)),axis=0)\n",
    "\n",
    "    return trn_data, trn_labels, tst_data, tst_labels\n",
    "\n",
    "# this function will score the differentially private classifier\n",
    "def test_dp_clf(data_tst, labels_tst, dp_clf):\n",
    "    import numpy as np\n",
    "    \n",
    "    # loop throught the data and record wrong answers\n",
    "    n = len(labels_tst)\n",
    "    tot_err = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        dp_ans = np.dot(data_tst[i,:], dp_clf)\n",
    "        if labels_tst[i] < 0:\n",
    "            if dp_ans >= 0:\n",
    "                tot_err += 1\n",
    "        else:\n",
    "            if dp_ans <= 0:\n",
    "                tot_err += 1\n",
    "    accuracy = (n - tot_err) * 1.0 / (n * 1.0)\n",
    "    return accuracy\n",
    "\n",
    "# This function is used give the pipeline interactive control.\n",
    "def svm_pipeline(Epsilon_pca = 0.5, Epsilon_svm = 0.5):\n",
    "    from sklearn import svm\n",
    "    import numpy as np\n",
    "    import dp_stats as dps\n",
    "    \n",
    "    # first generate the training and testing data\n",
    "    cls1, cls2, A = gen_data(num_tst_samp = 2200, k = 4)\n",
    "    \n",
    "    trn_data, trn_labels, tst_data, tst_labels = sample_selection(cls1, cls2, trn_size = 1000, N = 1100)\n",
    "    \n",
    "    # go through the non-differentially private PCA routine\n",
    "    sigma_control = np.dot(trn_data.T, trn_data)     # [d x d] = [d x Sample_size] [Sample_size x d]\n",
    "    U, S, V = np.linalg.svd(sigma_control)\n",
    "    \n",
    "    # grab the first k columns\n",
    "    U_reduc = U[:, :4]      # [d x k]\n",
    "    \n",
    "    # project the data onto the k subspace\n",
    "    trn_data_reduc = np.dot(trn_data, U_reduc)\n",
    "    \n",
    "    # go through SVM routine\n",
    "    clf = svm.SVC(kernel = 'linear', gamma = 0.01, C = 10)\n",
    "    clf.fit(trn_data_reduc, trn_labels)\n",
    "    \n",
    "    # reduce testing data to use to score the control classifier\n",
    "    tst_data_reduc = np.dot(tst_data, U_reduc)    # [d x k]\n",
    "    control_score = clf.score(tst_data_reduc, tst_labels)\n",
    "    #print(control_score)\n",
    "    \n",
    "    # go through differentially private pipeline\n",
    "    # dp_pca_sn ( data, epsilon=1.0 )  // samples must be in columns\n",
    "    sigma_dp = dps.dp_pca_ag(tst_data.T, epsilon = Epsilon_pca)\n",
    "    U_dp, S_dp, V_dp = np.linalg.svd(sigma_dp)\n",
    "    \n",
    "    # grab the first k columns\n",
    "    U_dp_reduc = U_dp[:, :4]\n",
    "    \n",
    "    # project the data\n",
    "    dp_trn_data = np.dot(trn_data, U_dp_reduc)\n",
    "    \n",
    "    # go through differentially private svm routine\n",
    "    # dp_svm(data, labels, method='obj', epsilon=0.1, Lambda = 0.01, h = 0.5)\n",
    "    clf_dp = dps.dp_svm(dp_trn_data, trn_labels, epsilon = Epsilon_svm)\n",
    "    \n",
    "    # reduce the testing data\n",
    "    tst_dp_data = np.dot(tst_data, U_dp_reduc)\n",
    "    \n",
    "    # test the differentially private classifier\n",
    "    dp_score = test_dp_clf(tst_dp_data, tst_labels, clf_dp)\n",
    "    #print(dp_score)\n",
    "    \n",
    "    # output the results\n",
    "    control_txt = \"Non-private Quality: {}\".format(round(control_score, 4))\n",
    "    display(control_txt)\n",
    "    dp_txt = \"Differentially Private Quality: {}\".format(round(dp_score, 4))\n",
    "    display(dp_txt)\n",
    "    \n",
    "interact(svm_pipeline, Epsilon_pca=(0.01,1.0,0.05), Epsilon_svm=(0.01,1.0, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentially Private LR Algorithm Tutorial\n",
    "\n",
    "\n",
    "The following tutorial gives one example of how a differentially private pipeline of PCA and LR funcitons are called. The data set used is generated from randomly drawn samples from a mulitivariate Gaussian distribution. A non-differentially private pipeline of PCA and LR is also  completed. This is used to compute the accuracy of the differentially private pipeline.\n",
    "\n",
    "###### A sample of the pipeline is shown below:\n",
    "\n",
    "Generate samples --> Perform PCA for dimension reduction --> Perform LR for classifier training --> Test classifier\n",
    "\n",
    "The parameters that can be adjusted are:\n",
    "\n",
    "- Epsilon_pca\n",
    "- Epsilon_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.885\n",
      "0.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Non-private Quality: 0.885'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Differentially Private Quality: 0.91'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This tutorial shows one common way to implement a logistic regression pipeline\n",
    "\n",
    "# This funciton is used for an interactive pipeline\n",
    "def lr_pipeline(Epsilon_pca = 0.5, Epsilon_svm = 0.5):\n",
    "    import numpy as np\n",
    "    import dp_stats as dps\n",
    "    from sklearn import linear_model\n",
    "    \n",
    "    # go through non-differentially private pipeline first\n",
    "    \n",
    "    # first generate the training and testing data\n",
    "    cls1, cls2, A = gen_data(num_tst_samp = 2200, k = 4)\n",
    "    \n",
    "    trn_data, trn_labels, tst_data, tst_labels = sample_selection(cls1, cls2, trn_size = 1000, N = 1100)\n",
    "    \n",
    "    # go through the non-differentially private PCA routine\n",
    "    sigma_control = np.dot(trn_data.T, trn_data)     # [d x d] = [d x Sample_size] [Sample_size x d]\n",
    "    U, S, V = np.linalg.svd(sigma_control)\n",
    "    \n",
    "    # grab the first k columns\n",
    "    U_reduc = U[:, :4]      # [d x k]\n",
    "    \n",
    "    # project the data onto the k subspace\n",
    "    trn_data_reduc = np.dot(trn_data, U_reduc)\n",
    "    \n",
    "    # go through logistic regression routine\n",
    "    lr_control = linear_model.LogisticRegression(C=1e5)\n",
    "    lr_control.fit(trn_data_reduc, trn_labels)\n",
    "    \n",
    "    # reduce testing data to use to score the control classifier\n",
    "    tst_data_reduc = np.dot(tst_data, U_reduc)    # [d x k]\n",
    "    control_score = lr_control.score(tst_data_reduc, tst_labels)\n",
    "    print(control_score)\n",
    "    \n",
    "    # go through differentially private pipeline\n",
    "    # dp_pca_sn ( data, epsilon=1.0 )  // samples must be in columns\n",
    "    sigma_dp = dps.dp_pca_ag(tst_data.T, epsilon = Epsilon_pca)\n",
    "    U_dp, S_dp, V_dp = np.linalg.svd(sigma_dp)\n",
    "    \n",
    "    # grab the first k columns\n",
    "    U_dp_reduc = U_dp[:, :4]\n",
    "    \n",
    "    # project the data\n",
    "    dp_trn_data = np.dot(trn_data, U_dp_reduc)\n",
    "    \n",
    "    # go through differentially private lr routine \n",
    "    # dp_lr(data, labels, method='obj', epsilon=0.1, Lambda = 0.01 )\n",
    "    lr_dp = dps.dp_lr(dp_trn_data, trn_labels, epsilon = Epsilon_svm)\n",
    "    \n",
    "    # reduce the testing data\n",
    "    tst_dp_data = np.dot(tst_data, U_dp_reduc)\n",
    "    \n",
    "    # test the differentially private classifier\n",
    "    dp_score = test_dp_clf(tst_dp_data, tst_labels, lr_dp)\n",
    "    print(dp_score)\n",
    "    \n",
    "    # output the results\n",
    "    control_txt = \"Non-private Quality: {}\".format(round(control_score, 4))\n",
    "    display(control_txt)\n",
    "    dp_txt = \"Differentially Private Quality: {}\".format(round(dp_score, 4))\n",
    "    display(dp_txt)\n",
    "\n",
    "interact(lr_pipeline, Epsilon_pca=(0.01,1.0,0.05), Epsilon_svm=(0.01,1.0, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  $(document).ready(function(){\n",
       "    $('div.prompt').hide();\n",
       "    $('div.back-to-top').hide();\n",
       "    $('nav#menubar').hide();\n",
       "    $('.breadcrumb').hide();\n",
       "    $('.hidden-print').hide();\n",
       "  });\n",
       "</script>\n",
       "\n",
       "<footer id=\"attribution\" style=\"float:right; color:#999; background:#fff;\">\n",
       "Created with Jupyter, delivered by Fastly, rendered by Rackspace.\n",
       "</footer>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#999; background:#fff;\">\n",
    "Created with Jupyter, delivered by Fastly, rendered by Rackspace.\n",
    "</footer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
