{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief introduction to differential privacy\n",
    "---\n",
    "\n",
    "The goal of these tutorials is to give a hands-on introduction to *differential privacy*, a framework for thinking about the privacy risks inherent when doing statistics or data analytics on private or sensitive data. Many approaches to protecting data privacy seek to \"anonymize\" the data by removing obvious (or not so obvious) identifiers. For example, a data set might have names, addresses, social security numbers, and other personally identifying information removed. However, that does not guarantee that publishing a stripped-down data set is still safe -- there have been many well-publicized attacks on supposedly \"sanitized\" data that use a small amount of auxiliary (and sometimes public) information to re-identify individuals in the data set.\n",
    "\n",
    "The fundamental difficulty in these examples is that the *data itself is uniquely identifying*. The follow-on implication is that if we publish the output of a program (say, a statistical analysis method) that runs on private data, we *reveal something about the individuals in the data*. The *differential privacy* model is a way to quantify this additional risk of re-identification. Privacy is a property of the *algorithm that operates on the data*; different algorithms incur different *privacy risks*.\n",
    "\n",
    "Differential privacy was first proposed in a paper by Dwork, McSherry, Nissim, and Smith in 2006 [DMNS06]. In the intervening years there has been a rapid growth in the research literature on differentially private approaches for many statistical, data mining, and machine learning algorithms of interest. The goal of this package is to provide easy-to-use implementations of these methods as well as tutorials (via ipython notebooks) to show how to use these methods.\n",
    "\n",
    "## Definition of Differential Privacy\n",
    "\n",
    "An algorithm $\\mathcal{A}$ taking values in a set $S$ provides $(\\epsilon,\\delta)$-differential privacy if\n",
    "$$\\text{Pr}(\\mathcal{A}(D) \\in S) \\leq e^{\\epsilon} \\text{Pr}(\\mathcal{A}(D') \\in S) + \\delta$$\n",
    "for all measurable $S \\subseteq \\mathcal{S}$ and all data sets $D$ and $D'$ differing in a single entry [DR14]. \n",
    "\n",
    "This definition essentially states that the probability of the output of an algorithm is not changed significantly if the corresponding database input is changed by just one entry. Here, $\\epsilon$ and $\\delta$ are privacy parameters, where low $\\epsilon$ and $\\delta$ ensure more privacy. It should be noted here that the parameter $\\delta$ can be interpreted as the probability that the algorithm fails. Therefore, an $(\\epsilon,0)$-differentially private algorithm guarantees much stronger privacy than an $(\\epsilon,\\delta)$-differentially private algorithm, where $\\delta > 0$. We refer to $(\\epsilon,0)$ differential privacy as $\\epsilon$-differential privacy.\n",
    "\n",
    "### Why do we need differential privacy?\n",
    "\n",
    "Consider a database of salaries of 5 people and an algorithm $f$ that outputs the average salary of the database. Let us assume that there is an adversary who can only observe the output of the algorithm. To be more specific, consider the following collection of salaries\n",
    "\n",
    "$$X = [100 \\ \\ 120 \\ \\ 110 \\ \\ 130 \\ \\ 140]\\ \\ \\Rightarrow\\ \\ f(X) = 120$$\n",
    "\n",
    "Now, let us assume that we add another individual to our collection of salaries and his/her salary is 1000. If we compute the output of the algorithm, we, along with the adversary, would observe that the average salary has significantly increased - indicating that there is a high salary drawing person in the database. \n",
    "\n",
    "$$ X' = [100 \\ \\ 120 \\ \\ 110 \\ \\ 130 \\ \\ 140 \\ \\ 1000]\\ \\ \\Rightarrow\\ \\ f(X') = 266.67$$\n",
    "\n",
    "This situation maybe unwanted to the individuals in the collection. Differential privacy *modifies* the algorithm in such a way that this difference in the output of the algorithm is suppressed. More formally, we are interested in the *sensitivity* of the function in consideration and we need to add noise scaled to the sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "\n",
    "[DMNS06] Dwork, C., McSherry, F., Nissim, K., and Smith, A. (2006). “Calibrating noise to sensitivity in private data analysis,” in Theory of Cryptography. Lecture notes in computer science, Vol. 3876, eds S. Halevi and T. Rabin (Berlin, Heidelberg: Springer), 265–284.\n",
    "\n",
    "[DR14] Dwork, C., and Roth, A. (2014). The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 9(3-4), 211-407."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
